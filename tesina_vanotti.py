# -*- coding: utf-8 -*-
"""TESINA - Vanotti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZkHilH2UhIQkZcdMala4EYqOm4hBnvov
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# Visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(rc={'figure.figsize': (12, 7)}) #para estandarizar el tama√±o de todos los gr√°ficos
sns.set_theme() # aplica el tema est√°ndar visual de los gr√°ficos
# %matplotlib inline
#para que al exportar el c√≥digo los gr√°ficos queden bien presentados
# Machine Learning
import xgboost as xgb
from sklearn.model_selection import train_test_split
# M√©tricas de evaluaci√≥n de mmodelo
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# EVT --- Distribuci√≥n Generalizada de Pareto para la cola de la distribuci√≥n
from scipy.stats import genpareto, lognorm
from scipy.optimize import bisect #m√©todo de bisecci√≥n para encontrar ra√≠ces de ecuaciones num√©ricamente (en este caso, para encontrar el VaR)
# Utilidad de ejes
from matplotlib.ticker import FuncFormatter #funci√≥n personalizada para formatear los n√∫meros de los ejes en los gr√°ficos
# reproducibilidad simple
import random
def set_seed(seed=123):
    random.seed(seed)
    np.random.seed(seed)

from sklearn.metrics import mean_squared_error, mean_absolute_error

!pip install category-encoders
from category_encoders import TargetEncoder

"""# üìä Preparaci√≥n de los datos"""

df = pd.read_csv("/content/dataset_tesis.csv")
df = pd.DataFrame(df)
df.head()

df.shape

df.isna().sum()

"""Columnas con NaN: INCOME, YOJ, HOME_VAL, OCCUPATION, CAR_AGE"""

# Limpieza de caracteres especiales
df = df.applymap(lambda x: x.replace('$', '') if isinstance(x, str) else x)
df = df.replace(to_replace=r'^z_', value='', regex=True)
df = df.applymap(lambda x: x.replace(',', '') if isinstance(x, str) else x)
df

# Conversi√≥n de columnas num√©ricas: si la columna es categ√≥rica, intento convertirla a float.
for i in df.columns:
    if df[i].dtypes == "object":
        try:
            df[i] = df[i].astype(float)
        except:
            pass

# me creo un DF con frecuencia y severidad
df_freq_sev = df.groupby('ID').agg({'CLAIM_FLAG': 'sum', 'CLM_AMT': 'mean'}) #agrupo todo por ID, o sea por p√≥liza
# o sea para las p√≥lizas que tienen m√°s de un registro agrupo y uso sus valores promedios
# CLAIM FLAG: ejemplo, si un cliente tiene 3 registros y 1 siniestro ‚Üí frecuencia = 1/3 = 0.33.
# CLM_AMT: monto promedio de siniestro
df_freq_sev = df_freq_sev.reset_index()
df_freq_sev.columns  = ['ID', 'FREQUENCY', 'SEVERITY']
df_freq_sev

#  Combino mi df agrupado por poliza con mi df inicial
df_merged = df.drop(columns = ['CLAIM_FLAG', 'CLM_AMT'])

# Then merge with the df_freq_sev
df_final = pd.merge(left = df_freq_sev, right=df_merged, how = 'left', on = 'ID')
df_final

# Saco los duplicados
df_final.drop_duplicates(inplace= True)
df_final.info()

# Imputo los NaN
# Uso la mediana para num√©ricas y "Desconocido" para categ√≥ricas.
for col in df_final.columns:
    if df_final[col].dtype in ["float64", "int64"]:
        df_final[col] = df_final[col].fillna(df_final[col].median())
    else:
        df_final[col] = df_final[col].fillna(df_final[col].mode())
df_final.isna().sum()

df_final.shape

from scipy.stats import pearsonr, spearmanr
# --- util: correlation ratio (eta) para categ√≥rica vs num√©rica ---
def correlation_ratio_cat_num(categories, values):
    # categories: Serie categ√≥rica (str/object/category)
    # values: Serie num√©rica
    # devuelve eta en [0,1]
    cat = pd.Categorical(categories)
    y = pd.Series(values).astype(float)
    mask = (~pd.isna(cat.codes)) & (~y.isna())
    codes = cat.codes[mask]
    y = y[mask]
    overall_mean = y.mean()
    # between-groups
    ss_between = 0.0
    for k in range(len(cat.categories)):
        yk = y[codes == k]
        if len(yk) > 0:
            ss_between += len(yk) * (yk.mean() - overall_mean) ** 2
    # total
    ss_total = ((y - overall_mean) ** 2).sum()
    if ss_total <= 0:
        return np.nan
    eta = np.sqrt(ss_between / ss_total)
    return float(eta)
def correlations_vs_targets_simple(df, targets=("FREQUENCY","SEVERITY")):
    """Calcula y consolida el coeficiente final (Pearson o Eta) por feature."""

    # 1. Separar tipos de columnas
    num_cols = [c for c in df.columns if c not in targets and pd.api.types.is_numeric_dtype(df[c])]
    cat_cols = [c for c in df.columns if c not in targets and not pd.api.types.is_numeric_dtype(df[c])]

    out_rows = []

    for tgt in targets:
        y = df[tgt]

        # 2. Num√©ricas: Coeficiente Pearson
        for col in num_cols:
            x = df[col]
            mask = ~(x.isna() | y.isna())
            r = np.nan
            if mask.sum() >= 3:
                r, _ = pearsonr(x[mask], y[mask])

            # Almacenar el resultado en el campo 'score'
            out_rows.append({"feature": col, "target": tgt, "score": r})

        # 3. Categ√≥ricas: Coeficiente Eta
        for col in cat_cols:
            eta = correlation_ratio_cat_num(df[col], y) # Asume que esta funci√≥n est√° definida

            # Almacenar el resultado en el campo 'score'
            out_rows.append({"feature": col, "target": tgt, "score": eta})

    # Devolver un DataFrame con solo feature, target, y score
    res = pd.DataFrame(out_rows)
    return res
corr_table = correlations_vs_targets_simple(df_final, targets=("FREQUENCY","SEVERITY"))
# === PROCESAMIENTO FINAL Y SALIDA ===

# 1. Seleccionar y Ordenar para FREQUENCY
top_freq = (corr_table[corr_table["target"]=="FREQUENCY"]
            .sort_values("score", ascending=False))

# 2. Seleccionar y Ordenar para SEVERITY
top_sev = (corr_table[corr_table["target"]=="SEVERITY"]
            .sort_values("score", ascending=False))

# --- SALIDA M√çNIMA REQUERIDA ---

# Salida para FREQUENCY
print("\n=== TOP CORRELATIONS VS. FREQUENCY ===")
# Muestra solo el nombre de la columna y el coeficiente (score)
for _, row in top_freq[["feature", "score"]].head(10).iterrows():
    print(f"{row['feature']}: {row['score']:.4f}")

# Salida para SEVERITY
print("\n=== TOP CORRELATIONS VS. SEVERITY ===")
# Muestra solo el nombre de la columna y el coeficiente (score)
for _, row in top_sev[["feature", "score"]].head(10).iterrows():
    print(f"{row['feature']}: {row['score']:.4f}")

def hist_num_vs_num(df, var_x, var_y, gridsize=20):
    plt.figure(figsize=(5, 4))
    plt.hexbin(df[var_x], df[var_y], gridsize=gridsize, cmap="ocean", mincnt=1)
    plt.colorbar(label='Densidad')
    plt.xlabel(var_x)
    plt.ylabel(var_y)
    plt.title(f"Densidad conjunta: {var_x} vs {var_y}")
    plt.tight_layout()
    plt.show()
hist_num_vs_num(df_final, "AGE", "FREQUENCY")

def hist_num_vs_cat(df, var_num, var_cat, bins=20):
    plt.figure(figsize=(6, 4))
    categorias = df[var_cat].dropna().unique()

    for cat in categorias:
        subset = df[df[var_cat] == cat][var_num].dropna()
        plt.hist(subset, bins=bins, alpha=0.6, label=f"{var_cat}={cat}")

    plt.title(f"Distribuci√≥n de {var_num} seg√∫n {var_cat}")
    plt.xlabel(var_num)
    plt.ylabel("Frecuencia")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

hist_num_vs_num(df_final, "AGE", "SEVERITY")

hist_num_vs_num(df_final, "MVR_PTS", "SEVERITY")

# columnas con mayor poder explicativo
columnas_deseadas = [
    'MVR_PTS',
    'CLM_FREQ',
    'OCCUPATION',
    'EDUCATION',
    'PARENT1',
    'REVOKED',
    'MSTATUS',
    'OLDCLAIM',
    'HOMEKIDS',
    'CAR_USE',
    'AGE',
    'FREQUENCY',
    'SEVERITY',
    'ID'
]

df_final = df_final[columnas_deseadas]
print("Columnas en el nuevo df_final:")
print(df_final.columns.tolist())
print(f"Nueva forma del DataFrame: {df_final.shape}")

"""TRAIN TEST SPLIT

"""

train_data, test_data = train_test_split(df_final, test_size=0.2, random_state=42)

def create_xgb_model(params):
    return xgb.XGBRegressor(**params)

"""-------------------------------------------------

# **MODELO DE FRECUENCIA**
"""

import matplotlib.ticker as ticker
counts = df_final['FREQUENCY'].value_counts().sort_index()
plt.style.use('seaborn-v0_8-whitegrid')

plt.figure(figsize=(9, 5))

plt.bar(
    counts.index,
    counts.values,
    width=0.8,
    color='lightblue',
    edgecolor='black'
)

plt.xticks(
    counts.index, # Posiciones de los ticks (centradas autom√°ticamente)
    labels=counts.index.astype(int) # Los labels son los valores de frecuencia
)

# 5. Etiquetas y T√≠tulos Acad√©micos
plt.title(
    'Distribuci√≥n Emp√≠rica de la Frecuencia de Siniestros',
    fontsize=16,
    fontweight='bold'
)
plt.xlabel('Frecuencia de Siniestros Observada', fontsize=12)
plt.ylabel('Conteo de Observaciones', fontsize=12)
plt.xlim(left=-0.5, right=counts.index.max() + 0.5)

plt.tight_layout()
plt.show()

def preprocess_for_freq(train_data, test_data):
    # 1) X / y
    X_train = train_data.drop(['FREQUENCY', 'ID', 'SEVERITY'], axis=1).copy() #elimino todas las columnas que no se usan como explicativas del modelo
    y_train = train_data['FREQUENCY'].copy()

    X_test  = test_data.drop(['FREQUENCY', 'ID', 'SEVERITY'], axis=1).copy()
    y_test  = test_data['FREQUENCY'].copy()
#variables categ√≥ricas vs. num√©ricas
    cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist() #columnas explicativas categ√≥ricas
    num_cols = [c for c in X_train.columns if c not in cat_cols] #columnas explicativas num√©ricas

#Target Encoding SOLO en categ√≥ricas.
# lo que hace el target encoding es que dentro de los distintos valores que puede tomar una columna, al valor categ√≥rico que tiene mayor frecuencia lo reemplaza un n√∫mero mayor.
    if len(cat_cols) > 0:
        encoder = TargetEncoder(
            cols=cat_cols,
            smoothing=10,            # suaviza vs media global
            min_samples_leaf=50,     # evita overfitting
            handle_unknown='value',
            handle_missing='value'
        )
        X_train_cat = encoder.fit_transform(X_train[cat_cols], y_train)
        X_test_cat  = encoder.transform(X_test[cat_cols])
    else:
        X_train_cat = pd.DataFrame(index=X_train.index)
        X_test_cat  = pd.DataFrame(index=X_test.index)

    # Combino num√©ricas + categ√≥ricas codificadas reemplazadas por n√∫meros
    X_train_encoded = pd.concat(
        [X_train[num_cols].reset_index(drop=True),
         X_train_cat.reset_index(drop=True)],
        axis=1
    )
    X_test_encoded = pd.concat(
        [X_test[num_cols].reset_index(drop=True),
         X_test_cat.reset_index(drop=True)],
        axis=1
    )

    return X_train_encoded, X_test_encoded, y_train, y_test


def train_test_freq(model, train_data, test_data):
    X_train, X_test, y_train, y_test = preprocess_for_freq(train_data, test_data) #llama a la funci√≥n que prepara los datos
    # Control para ver si el split train/test qued√≥ balanceado --- (que no haya diferencia grande entre las medias)
    print('--- Validaci√≥n de la Divisi√≥n en train y test (y) ---')
    print(f'Media y_train: {y_train.mean():.6f}')
    print(f'Media y_test:  {y_test.mean():.6f}')
    print('-------------------------------------')
# Entrenamiento del modelo
    model_fit = model.fit(X_train, y_train) #entreno/ajusto el modelo con un regresor, en este caso XGBoost
    y_pred = model_fit.predict(X_test) #Genera predicciones de frecuencia para los datos de TEST
#--- M√©tricas
    bias_pct = (y_pred.mean() - y_test.mean()) / max(y_test.mean(), 1e-12) #m√©trica para ver si se sobreestima o subestima la media
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = (mean_absolute_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f'Sesgo de la media: {100*bias_pct:.3f}%')
    print(f'RMSE: {rmse:.6f}')
    print(f'MAE: {mae:.6f}')
    print(f'R¬≤: {r2:.4f}')

    return model_fit, y_pred, y_test

xgb_freq_params = {
    'objective': 'count:poisson',  # para FREQUENCY en [0,1]
    'learning_rate': 0.05,
    'max_depth': 5,
    'n_estimators': 250,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0,
    'random_state': 42
}
xgb_freq_model = create_xgb_model(xgb_freq_params)

model_freq, preds_freq, y_test_freq = train_test_freq(xgb_freq_model, train_data, test_data)
print('--- Valor esperado real de la frecuencia de una p√≥liza ---')
print(f'Media real: {df_final['FREQUENCY'].mean():.6f}')

df_completo = test_data.copy()

# Convertimos a numpy (garantiza forma 1D)
y_real = np.array(y_test_freq).ravel()
y_pred = np.array(preds_freq).ravel()

# Agregamos columnas
df_completo["y_real"] = y_real
df_completo["y_predicho"] = y_pred
df_completo["error_abs"] = abs(df_completo["y_real"] - df_completo["y_predicho"])
df_completo["error_rel"] = df_completo["error_abs"] / (df_completo["y_real"] + 1e-6)

# Vista preliminar
print(df_completo.head())

valor_esp_frecuencia = preds_freq.mean()
print(f'Valor esperado de frecuencia de una p√≥liza: {valor_esp_frecuencia:.6f}')

df_freq = pd.DataFrame({
    "y_true": y_test_freq,
    "y_pred": preds_freq
})
df_freq["decile"] = pd.qcut(df_freq["y_pred"], 10, duplicates="drop")
calib = df_freq.groupby("decile")[["y_true","y_pred"]].mean().reset_index()
print(calib)

# Los datos proporcionados se ingresan en un DataFrame de pandas
df = pd.DataFrame(calib)
df['decil_idx'] = range(1, len(df) + 1) # Crear un √≠ndice num√©rico para el eje X

# ----------------------------------------------------------------------

#Comparaci√≥n de y_true vs y_pred por Decil (Calibraci√≥n y Discriminaci√≥n)

plt.figure(figsize=(8, 6))
sns.lineplot(x='decil_idx', y='y_true', data=df, marker='o', label=r'Valor Real Promedio ($\mathbf{y_{true}}$)', color='tab:blue', linewidth=2)
sns.lineplot(x='decil_idx', y='y_pred', data=df, marker='x', label=r'Valor Predicho Promedio ($\mathbf{y_{pred}}$)', color='tab:red', linestyle='--', linewidth=2)

plt.title('Calibraci√≥n del Modelo por Decil', fontsize=16)
plt.xlabel('Decil (basado en $\mathbf{y_{pred}}$)', fontsize=12)
plt.ylabel('Valor Promedio', fontsize=12)
plt.xticks(df['decil_idx']) # Asegurar que las marcas son los n√∫meros de decil
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()

"""Ya tengo el modelo de frecuencia de mi cartera, con un valor esperado de 0,3566

hablar sobre MAE entre 0,3 y 0,6 Error aceptable en datos con conteos bajos
"""

plt.style.use('ggplot')
cols_posibles_drop = [c for c in ['FREQUENCY','SEVERITY','ID'] if c in train_data.columns]
feat_names_freq = [c for c in train_data.columns if c not in cols_posibles_drop]

imp = getattr(model_freq, "feature_importances_", None)
if imp is None:
    print("El modelo de frecuencia no expone feature_importances_.")
else:
    # Si el modelo fue entrenado con columnas transformadas, puede no coincidir 1 a 1.
    # Tomamos el m√≠nimo com√∫n para evitar errores.
    k = min(len(imp), len(feat_names_freq))
    pares = sorted(list(zip(feat_names_freq[:k], imp[:k])), key=lambda x: x[1], reverse=True)[:10]
    labels, vals = zip(*pares) if pares else ([], [])

    plt.figure(figsize=(7,4))
    y = np.arange(len(labels))

    # Gr√°fico de barras horizontales con el nuevo estilo
    plt.barh(y, vals, color='teal') # A√±ad√≠ un color para mejorar la visualizaci√≥n

    plt.yticks(y, labels)
    plt.gca().invert_yaxis()
    plt.xlabel("Importancia")
    plt.title("Top-10 variables predictoras ‚Äî Modelo de Frecuencia (XGB)")
    plt.tight_layout()
    plt.show()

# Restablecer el estilo predeterminado despu√©s de mostrar el gr√°fico (opcional, pero buena pr√°ctica)
plt.style.use('default')

"""# MODELO DE SEVERIDAD"""

df_final['SEVERITY'].max()

# Al graficar mis datos, vemos que la severidad toma la forma de una curva continua y sesgada a la derecha, lo que la hace compatible con una distribucion Gamma.
#df_final[df_final['SEVERITY'] != 0]['SEVERITY'].hist(bins = 100, range=(0, 46000))

plt.style.use('seaborn-v0_8-whitegrid')


plt.figure(figsize=(10, 6))


plt.hist(
    df_final[df_final['SEVERITY'] != 0]['SEVERITY'],
    bins=100,
    range=(0, 46000),
    color='lightblue',
    edgecolor='black',
    alpha=0.7
)

formatter = ticker.StrMethodFormatter('{x:,.0f}') # Formato con separadores de miles y cero decimales
plt.gca().xaxis.set_major_formatter(formatter)

plt.title(
    'Distribuci√≥n de la severidad de siniestros observados',
    fontsize=16,
    fontweight='bold'
)
plt.xlabel('Monto de la severidad', fontsize=12)
plt.ylabel('Frecuencia', fontsize=12)

# Rotar las etiquetas del eje X para mejor lectura de n√∫meros grandes
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

def preprocess_for_sev(train_data, test_data):
    # Filtro ceros (solo positivos para severidad)
    train_nonzero = train_data[train_data['SEVERITY'] > 0].copy()
    test_nonzero  = test_data[test_data['SEVERITY'] > 0].copy()

    #  X / y
    X_train = train_nonzero.drop(['FREQUENCY', 'ID', 'SEVERITY'], axis=1)
    y_train = train_nonzero['SEVERITY']

    X_test  = test_nonzero.drop(['FREQUENCY', 'ID', 'SEVERITY'], axis=1)
    y_test  = test_nonzero['SEVERITY']

    # Categ√≥ricas vs num√©ricas (ya ven√≠an imputadas)
    cat_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()
    num_cols = [c for c in X_train.columns if c not in cat_cols]

    # Target Encoding SOLO en categ√≥ricas
    if len(cat_cols) > 0:
        encoder = TargetEncoder(
            cols=cat_cols, smoothing=10, min_samples_leaf=50,
            handle_unknown='value', handle_missing='value'
        )
        X_train_cat = encoder.fit_transform(X_train[cat_cols], y_train)
        X_test_cat  = encoder.transform(X_test[cat_cols])   # <- sin y_test
    else:
        X_train_cat = X_train[[]].copy()
        X_test_cat  = X_test[[]].copy()

    # matrices finales
    X_train_enc = pd.concat([X_train[num_cols].reset_index(drop=True),
                             X_train_cat.reset_index(drop=True)], axis=1)
    X_test_enc  = pd.concat([X_test[num_cols].reset_index(drop=True),
                             X_test_cat.reset_index(drop=True)], axis=1)

    return X_train_enc, X_test_enc, y_train, y_test

def train_test_sev(model, train_data, test_data):
    X_train, X_test, y_train, y_test = preprocess_for_sev(train_data, test_data)

    print('--- Validaci√≥n de la Divisi√≥n (y) ---')
    print(f'Media y_train: {y_train.mean():.2f}')
    print(f'Media y_test:  {y_test.mean():.2f}')
    print('------------------------------------')

    model_fit = model.fit(X_train, y_train)
    y_pred = model_fit.predict(X_test)

    # Sesgo media
    bias = (1 - y_test.mean() / y_pred.mean())
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae  = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # M√©tricas ROBUSTAS (recortadas 99¬∫ pto para que no dominen 1-2 outliers)
    p99_y, p99_p = np.percentile(y_test, 99), np.percentile(y_pred, 99)
    mask = (y_test <= p99_y) & (y_pred <= p99_p)
    rmse_rob = np.sqrt(mean_squared_error(y_test[mask], y_pred[mask]))
    mae_rob  = mean_absolute_error(y_test[mask], y_pred[mask])

    print(f'Sesgo media: {bias:.4f}')
    print(f'RMSE: {rmse:,.2f} | MAE: {mae:,.2f}')
    print(f'RMSE(‚â§P99): {rmse_rob:,.2f} | MAE(‚â§P99): {mae_rob:,.2f}')
    print(f'R¬≤: {r2:.4f}')

    return model_fit, y_pred, y_test, X_test

xgb_sev_params = {
    'objective': 'reg:gamma',     # target severidad sigue una distribuci√≥n Gamma condicional a las covariables ùëã.
    'learning_rate': 0.05,
    'max_depth': 4,
    'n_estimators': 600,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0,
    'alpha': 30,
    'random_state': 42
}
xgb_sev_model = create_xgb_model(xgb_sev_params)
model_sev, preds_sev, y_test_sev, X_test_sev = train_test_sev(xgb_sev_model, train_data, test_data)
print('--- Valor esperado real de la severidad de una p√≥liza ---')
print(f'Media REAL: {df_final[df_final['SEVERITY'] != 0]['SEVERITY'].mean():.6f}')

eps = 1e-6
plt.figure(figsize=(7,6))
plt.scatter(np.log(y_test_sev+eps), np.log(preds_sev+eps), alpha=0.35, s=12)
mx = max(np.log(y_test_sev+eps).max(), np.log(preds_sev+eps).max())
plt.plot([0, mx],[0, mx],'r--',lw=1)
plt.xlabel("log(SEVERITY real)")
plt.ylabel("log(SEVERITY pred)")
plt.title("Severidad (Modelo XGBoost)")
plt.grid(alpha=0.3)
plt.show()

"""Este gr√°fico nos muestra que el modelo describe bien el ‚Äúpromedio‚Äù del comportamiento, pero pierde capacidad de ajuste en los siniestros de alto valor."""

# "Empirical Cumulative Distribution Function" - gr√°fico de probabilidad acumulada (a escala logar√≠tmica)
def ecdf(a):
    a = np.sort(a)
    return a, np.arange(1, a.size+1)/a.size

x_y, F_y = ecdf(np.log(y_test_sev+eps))
x_p, F_p = ecdf(np.log(preds_sev+eps))
plt.figure(figsize=(8,5))
plt.plot(x_y, F_y, label='Real (escala log)')
plt.plot(x_p, F_p, '--', label='Pred (escala log)')
plt.title("Probabilidad Acumulada")
plt.legend(); plt.grid(alpha=0.3); plt.show()

# "Empirical Cumulative Distribution Function" - gr√°fico de probabilidad acumulada
def ecdf(a):
    a = np.sort(a)
    return a, np.arange(1, a.size+1)/a.size

x_y, F_y = ecdf(y_test_sev+eps)
x_p, F_p = ecdf(preds_sev+eps)
plt.figure(figsize=(8,5))
plt.plot(x_y, F_y, label='Real')
plt.plot(x_p, F_p, '--', label='Pred')
plt.title("Probabilidad Acumulada")
plt.legend(); plt.grid(alpha=0.3); plt.show()

"""Se puede observar que mirando la distribuci√≥n acumulada real vs. predicha, el modelo capta correctamente la forma general de la distribuci√≥n, pero no reproduce adecuadamente la variabilidad y la dispersi√≥n de los valores que se encuentran en los extremos."""

print(f"Predicci√≥n M√≠nima del modelo XGBOOST: {preds_sev.min():.2f}")
print(f"Predicci√≥n M√°xima del modelo XGBOOST: {preds_sev.max():.2f}")
print(f"Valor m√°ximo real: {df_final['SEVERITY'].max():.2f}")

VaR_dict = {}
TVaR_dict = {}
for p in [0.95, 0.99, 0.995]:
    VaR_p = np.percentile(preds_sev, 100 * p)
    TVaR_p = preds_sev[preds_sev >= VaR_p].mean()
    VaR_dict[p] = VaR_p
    TVaR_dict[p] = TVaR_p
print(f"Medidas de riesgo del modelo XGBOOST:")
for p in [0.95, 0.99, 0.995]:

    print(f"{int(p*100)}%")
    print(f"  VaR = {VaR_dict[p]:.2f}")
    print(f"  TVaR = {TVaR_dict[p]:.2f}\n")

"""---------------------------------------------------------------------------

## **Modelo MIXTO: Machine Learning y Teor√≠a de Valores Extremos**

---------------------------------------------------------------------------

## üéØ SELECCI√ìN DEL THRESHOLD

**Mean Excess Plot**

e(u)=E[X‚àíu/X>u]

Si los datos en la cola realmente siguen una Generalized Pareto Distribution (GPD), el gr√°fico de e(u) en funci√≥n de u deber√≠a verse aproximadamente lineal a partir de cierto punto.

Ese ‚Äúpunto de quiebre‚Äù (cuando pasa de curvado a casi lineal) es el umbral candidato.
"""

from matplotlib.ticker import FuncFormatter, LogLocator
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def human_format(x, pos=None):
    if x >= 1e6: return f"{x/1e6:.1f}M"
    if x >= 1e3: return f"{x/1e3:.0f}K"
    return f"{x:.0f}"

# Datos
claims = df_final.loc[df_final['SEVERITY'] > 0, 'SEVERITY'].to_numpy()
claims = claims[np.isfinite(claims)]

def mean_excess_and_count(data, thresholds):
    mean_excesses, num_exceedances = [], []
    for u in thresholds:
        exc = data[data > u] - u
        num_exceedances.append(exc.size)
        mean_excesses.append(np.nan if exc.size == 0 else exc.mean())
    return mean_excesses, num_exceedances

# Umbrales
start_threshold = np.percentile(claims, 80)
end_threshold   = np.percentile(claims, 98)
thresholds = np.linspace(start_threshold, end_threshold, 500)

e_values, n_exceedances = mean_excess_and_count(claims, thresholds)

# --- Suavizado (media m√≥vil) ---
window = 15  # cantidad de puntos del promedio m√≥vil
e_smooth = pd.Series(e_values).rolling(window, center=True, min_periods=1).mean()

# --- Gr√°fico ---
fmt = FuncFormatter(human_format)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
xticks = np.linspace(start_threshold, end_threshold, 8)

# Mean Excess Plot (suavizado)
ax1.plot(thresholds, e_values, lw=1, alpha=0.4, color="tab:blue", label="Original")
ax1.plot(thresholds, e_smooth, lw=2.5, color="tab:blue", label="Suavizado")
ax1.set_xlabel("Umbral u"); ax1.set_ylabel("Exceso Medio e(u)")
ax1.set_title("Mean Excess Plot - Severidad")
ax1.grid(True, ls="--", alpha=0.6)
ax1.legend()
ax1.set_xticks(xticks)
ax1.xaxis.set_major_formatter(fmt)
ax1.yaxis.set_major_formatter(fmt)

# Conteo de excedencias
ax2.plot(thresholds, n_exceedances, lw=2, color="tab:red")
ax2.set_xlabel("Umbral u"); ax2.set_ylabel("N√∫mero de excedencias (log)")
ax2.set_title("Conteo de Excesos")
ax2.grid(True, ls="--", alpha=0.6)
ax2.set_yscale("log")
ax2.set_xticks(xticks)
ax2.xaxis.set_major_formatter(fmt)
ax2.yaxis.set_major_locator(LogLocator(base=10))
ax2.yaxis.set_major_formatter(fmt)

plt.tight_layout()
plt.show()

"""üëâ Antes del valor 10,000: *la curva es c√≥ncava y con pendiente creciente pronunciada* ‚Üí esa zona todav√≠a pertenece al cuerpo de la distribuci√≥n, no a la cola ‚Äúpura‚Äù. -->

üéØ A partir de ‚âà 10,000 - 11,000: **PUNTO DE QUIEBRE**. La pendiente comienza a estabilizarse y el crecimiento del exceso medio se vuelve casi lineal, aunque con peque√±as fluctuaciones debidas a la menor cantidad de observaciones.


El gr√°fico se aplaca y muestra fluctuaciones aleatorias y planas (una tendencia constante, alrededor de 14K)


Este es el rango de umbral √≥ptimo para aplicar la GPD. Indica que, para cualquier umbral u que elijas en esta zona (p. ej., $11K, $15K, $17K), la media del exceso sobre ese umbral permanece esencialmente constante (o ligeramente creciente).

*Basado en el gr√°fico, un umbral de alrededor de $11,000 ser√≠a un buen punto de partida.*

Este an√°lisis es una prueba de robustez del modelo EVT.

El modelo es sensible a la elecci√≥n del umbral $u$.

El umbral m√°s bajo ($u=7500$) resulta en el mayor riesgo (VaR y TVaR) debido a que su GPD tiene el $\xi$ m√°s alto, indicando una cola de distribuci√≥n de p√©rdidas extremadamente pesada. Si se elige este umbral, la estimaci√≥n del riesgo es la m√°s conservadora.

Los umbrales m√°s altos ($u =10000, u=11000$ y $u=15000$) producen estimaciones de VaR y TVaR similares entre s√≠, lo que sugiere que, para la poblaci√≥n de siniestros por encima de $10000$, la distribuci√≥n de p√©rdidas extremas es m√°s estable y ligera.


Para la toma de decisiones (por ejemplo, determinar una reserva de capital), un actuario o gestor de riesgos probablemente favorecer√≠a el umbral que ofrece los par√°metros de GPD m√°s estables, mientras que tambi√©n considerar√≠a el nivel de conservaci√≥n deseado.
"""

# --- UMBRAL ---
threshold = 9407.77
# --- SEPARACI√ìN CUERPO / COLA ---
body_claims = claims[claims <= threshold]
tail_claims = claims[claims > threshold]
#--- Stats √∫tiles para el informe
n_total = claims.size
n_tail  = tail_claims.size
pct_tail = 100 * n_tail / max(n_total, 1)
# ---------------------------------------------
# GR√ÅFICOS
# ---------------------------------------------
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

# --- Plot 1: Distribuci√≥n completa con umbral ---
bins_full = np.linspace(0, np.percentile(claims, 99.9), 300)  # ajustado al rango real de tus datos

sns.histplot(body_claims, bins=bins_full, kde=False, color='navy', label='Cuerpo', ax=ax1)
sns.histplot(tail_claims, bins=bins_full, kde=False, color='steelblue', label='Cola', ax=ax1)

ax1.axvline(x=threshold, color='sienna', linestyle='--', linewidth=2, label=f'Umbral (${threshold:,.0f})')
ax1.set_xlabel('Monto del Siniestro')
ax1.set_ylabel('Frecuencia')
ax1.set_title('Distribuci√≥n completa con umbral')
ax1.set_yscale('log')  # √∫til para ver la cola
ax1.grid(axis='y', alpha=0.9)
ax1.legend()
ax1.xaxis.set_major_formatter(FuncFormatter(human_format))
ax1.yaxis.set_major_formatter(FuncFormatter(human_format))
ax1.set_xlim(0, np.percentile(claims, 99.9))
# Nota en el gr√°fico con #excesos
ax1.text(0.98, 0.95, f'Excesos > u: {n_tail} ({pct_tail:.2f}%)',
         transform=ax1.transAxes, ha='right', va='top', fontsize=10,
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
# --- Plot 2: Vista ampliada de la cola ---
sns.histplot(tail_claims, bins=200, kde=True, color='steelblue', ax=ax2, stat='density', alpha=0.6)
ax2.set_xlabel('Monto del Siniestro')
ax2.set_ylabel('Densidad')
ax2.set_title(f'Distribuci√≥n de la cola (siniestros > ${threshold:,.0f})')
ax2.grid(axis='y', alpha=0.9)
ax2.xaxis.set_major_formatter(FuncFormatter(human_format))
ax2.set_xlim(threshold, np.percentile(claims, 99.9))

plt.tight_layout()
plt.show()

"""‚úÖ*Ajusto la distribuci√≥n de los valores extremos v√≠a PEAKS OVER THRESHOLD*"""

from scipy.stats import genpareto

def fit_gpd_excesses(x, u):
    # Excesos (X > u)
    y = np.asarray(x)[x > u] - u
    # fijo loc=0
    xi, loc, beta = genpareto.fit(y, floc=0.0) #GPD ajustada debe empezar en 0 (el exceso m√≠nimo).
    return xi, beta, y

def var_pot(p, xi, beta, u, p_u):
    # v√°lido SOLO si p > 1 - p_u (cuantil en la cola) --- recordar que solo podemos conocer el VaR si el percentil est√° dentro de la cola de dist.
    if p <= 1 - p_u:
        return np.nan
    if abs(xi) < 1e-8:  # l√≠mite xi‚Üí0
        return u + beta * np.log(p_u/(1-p))
    return u + (beta/xi) * ((p_u/(1-p))**xi - 1)

def tvar_pot(p, xi, beta, u, p_u):
    v = var_pot(p, xi, beta, u, p_u)
    if np.isnan(v):
        return np.nan
    if xi >= 1:
        return np.inf
    return u + (beta + (v - u)) / (1 - xi)

u = threshold
claims = df_final.loc[df_final["SEVERITY"] > 0, "SEVERITY"].to_numpy()
xi, beta, excesses = fit_gpd_excesses(claims, u)
p_u = (claims > u).mean()

print(f"xi={xi:.4f}, beta={beta:.2f}, p_u={p_u:.4f}")

for p in [0.95, 0.99, 0.995]:
    v = var_pot(p, xi, beta, u, p_u)
    es = tvar_pot(p, xi, beta, u, p_u)
    label = f"{int(p*1000)/10:.1f}%"
    print(f"\nNivel {label}")
    if np.isnan(v):
        print("  (El cuant√≠l est√° por debajo del umbral: usar el modelo del cuerpo / emp√≠rico)")
    else:
        print(f"  VaR  = {v:,.2f}")
        print(f"  TVaR = {es:,.2f}   (Œî={es - v:,.2f} ‚â• 0)")

candidates = [7500,10000, 11000, 15000]
rows = []
claims = df_final.loc[df_final["SEVERITY"]>0, "SEVERITY"].to_numpy()

for u_ in candidates:
    xi_, beta_, exc_ = fit_gpd_excesses(claims, u_)
    p_u_ = (claims > u_).mean()
    for p in [0.99, 0.995]:
        VaR_ = var_pot(p, xi_, beta_, u_, p_u_)
        TVaR_ = tvar_pot(p, xi_, beta_, u_, p_u_)
        rows.append({"u": u_, "p": p, "xi": xi_, "beta": beta_, "p_u": p_u_, "VaR": VaR_, "TVaR": TVaR_})
sens = pd.DataFrame(rows)
print(sens)

"""üëâ La brecha TVaR‚àíVaR ~ 15K es bastante estable entre 99% y 99.5% ‚Üí se√±al de cola ‚Äúno explosiva‚Äù.

üëâ Œæ ‚âà 0.021 > 0 pero peque√±o ‚Üí cola ligera (m√°s pesada que exponencial, mucho m√°s liviana que Pareto ‚Äúdura‚Äù).


üëâ El ajuste POT‚ÄìGPD con ùë¢=11,000 captura una cola moderadamente pesada y produce VaR/TVaR razonables y crecientes. Esto valida la necesidad del enfoque mixto: el ML ‚Äúpuro‚Äù comprim√≠a la cola; la GPD corrige esa subestimaci√≥n.
"""

# --- Visualizaci√≥n del ajuste GPD ---
x = np.linspace(excesses.min(), excesses.max(), 200)
pdf_gpd = genpareto.pdf(x, xi, loc=0, scale=beta)  # loc=0 para excesos

plt.figure(figsize=(10, 5))
sns.histplot(
    excesses, bins=50, stat='density', color='steelblue', alpha=0.6, label='Datos (excesos)'
)
plt.plot(x, pdf_gpd, 'r-', lw=2, label='Ajuste GPD')

# --- T√≠tulo y ejes ---
plt.title(f"Ajuste de la cola con GPD (umbral = ${threshold:,.0f})")
plt.xlabel('Exceso sobre el umbral')
plt.ylabel('Densidad')
plt.legend()
plt.grid(alpha=0.3)

# --- Texto con par√°metros ---
param_text = (
    f"Œæ (shape) = {xi:.4f}\n"
    f"Œ≤ (scale) = {beta:,.2f}\n"
    f"p·µ§ = {p_u:.4%}\n"
    f"n_excesos = {len(excesses)}"
)
plt.text(
    0.98, 0.97, param_text,
    transform=plt.gca().transAxes,
    ha='right', va='top',
    fontsize=10,
    bbox=dict(boxstyle='round', facecolor='white', alpha=0.7)
)

plt.show()

from scipy.stats import kstest

# === QQ-PLOT (excesos emp√≠ricos vs cuantiles te√≥ricos GPD) ===
ex = np.asarray(excesses)
ex_sorted = np.sort(ex)
n = len(ex_sorted)

# Probabilidades "de gr√°fico" (evitan 0 y 1): (i - 0.5)/n
p = (np.arange(1, n+1) - 0.5) / n

# Cuantiles te√≥ricos bajo GPD ajustada (loc=0 porque son excesos)
q_theo = genpareto.ppf(p, c=xi, scale=beta, loc=0.0)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.scatterplot(x=q_theo, y=ex_sorted, s=18)
mx = max(q_theo.max(), ex_sorted.max())
plt.plot([0, mx], [0, mx], 'r--', lw=1.5, label='y = x')
plt.xlabel('Cuantiles te√≥ricos GPD')
plt.ylabel('Cuantiles emp√≠ricos (excesos)')
plt.title('QQ-plot GPD sobre excesos')
plt.grid(alpha=0.3)
plt.legend()

# (Opcional) versi√≥n log-log para resaltar cola
# plt.xscale('log'); plt.yscale('log')

# === PP-PLOT (CDF te√≥rica vs CDF emp√≠rica) ===
F_theo = genpareto.cdf(ex_sorted, c=xi, scale=beta, loc=0.0)
F_emp  = np.arange(1, n+1) / (n+1)  # escalado cl√°sico

plt.subplot(1,2,2)
sns.scatterplot(x=F_theo, y=F_emp, s=18)
plt.plot([0,1], [0,1], 'r--', lw=1.5, label='y = x')
plt.xlabel('F_te√≥rica (GPD)')
plt.ylabel('F_emp√≠rica (excesos)')
plt.title('PP-plot GPD sobre excesos')
plt.grid(alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()

# === KS test (goodness-of-fit) sobre excesos ===
# H0: los excesos provienen de GPD(xi, beta)
ks_stat, ks_p = kstest(ex, lambda z: genpareto.cdf(z, c=xi, scale=beta, loc=0.0))
print(f"KS statistic: {ks_stat:.4f}  |  p-value: {ks_p:.4f}")

"""p-value = 0.4864 ‚Üí indica que no hay evidencia estad√≠sticamente significativa para rechazar la hip√≥tesis nula
(‚Äúlos excesos provienen de una distribuci√≥n GPD con tus par√°metros estimados‚Äù).
"""

# --- Emp√≠rico (XGB ‚Äúpuro‚Äù) sobre observados positivos ---
sev_pos = np.asarray(y_test_sev)[np.asarray(y_test_sev) > 0]
sev_pos.sort()

def var_empirico(arr, p):
    if len(arr) == 0: return np.nan
    return float(np.quantile(arr, p, interpolation="linear"))

def tvar_empirico(arr, p):
    if len(arr) == 0: return np.nan
    v = var_empirico(arr, p)
    exc = arr[arr >= v]
    if len(exc) == 0: return v
    return float(exc.mean())

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

ps = [0.99, 0.995, 0.999]
rows = []
for p in ps:
    var_xgb = var_empirico(sev_pos, p)
    tvar_xgb = tvar_empirico(sev_pos, p)

    if p <= 1 - p_u:
        # nivel de cola a√∫n no alcanzado: usar emp√≠rico (h√≠brido==xgb)
        var_h  = var_xgb
        tvar_h = tvar_xgb
    else:
        var_h  = var_pot(p, xi, beta, u, p_u)
        tvar_h = tvar_pot(p, xi, beta, u, p_u)

    rows.append({
        "p": p,
        "VaR_XGB": var_xgb,
        "VaR_H√≠brido": var_h,
        "Œî_VaR_%": 100.0 * (var_h - var_xgb) / var_xgb if np.isfinite(var_xgb) and var_xgb>0 else np.nan,
        "TVaR_XGB": tvar_xgb,
        "TVaR_H√≠brido": tvar_h,
        "Œî_TVaR_%": 100.0 * (tvar_h - tvar_xgb) / tvar_xgb if np.isfinite(tvar_xgb) and tvar_xgb>0 else np.nan
    })

tabla_var = pd.DataFrame(rows)
print(tabla_var)

def preprocess_for_body(train_data, test_data, u):
    # Filtrar SOLO cuerpo: 0 < SEVERITY ‚â§ u
    tr = train_data
    te = test_data

    X_train = tr.drop(['FREQUENCY','ID','SEVERITY'], axis=1)
    y_train = tr['SEVERITY']

    X_test  = te.drop(['FREQUENCY','ID','SEVERITY'], axis=1)
    y_test  = te['SEVERITY']

    # Categ√≥ricas encode
    cat_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()
    num_cols = [c for c in X_train.columns if c not in cat_cols]

    if len(cat_cols) > 0:
        enc = TargetEncoder(
            cols=cat_cols, smoothing=10, min_samples_leaf=50,
            handle_unknown='value', handle_missing='value'
        )
        X_train_cat = enc.fit_transform(X_train[cat_cols], y_train)
        X_test_cat  = enc.transform(X_test[cat_cols])
    else:
        enc = None
        X_train_cat = X_train[[]].copy()
        X_test_cat  = X_test[[]].copy()

    Xtr = pd.concat([X_train[num_cols].reset_index(drop=True),
                     X_train_cat.reset_index(drop=True)], axis=1)
    Xte = pd.concat([X_test[num_cols].reset_index(drop=True),
                     X_test_cat.reset_index(drop=True)], axis=1)

    # target en log
    ytr_log = np.log(y_train.values)
    yte      = y_test.values  # guardamos en escala original

    return Xtr, Xte, ytr_log, yte, enc, cat_cols, num_cols

def train_test_body(model, train_data, test_data, u):
    Xtr, Xte, ytr_log, yte, enc, cat_cols, num_cols = preprocess_for_body(train_data, test_data, u)

    # dentro de train_test_body, despu√©s de crear Xtr, ytr_log
    ytr = np.minimum(np.exp(ytr_log), u)  # escala original, truncado en u por definici√≥n del cuerpo
    # k controla cu√°nto ‚Äúempuj√°s‚Äù la zona alta (3‚Äì6 suele andar bien)
    k = 4
    w = (ytr / u) ** k           # en [0,1]; cerca de u ‚Üí peso alto
    w = np.clip(w, 0.1, 10.0)    # l√≠mites sanos para estabilidad
    model_fit = model.fit(Xtr, ytr_log, sample_weight=w)
    ypred_log = model_fit.predict(Xte)

    # Correcci√≥n de back-transform (lognormal): E[Y] ‚âà exp(mu + 0.5*sigma^2)
    res = ytr_log - model_fit.predict(Xtr)
    sigma2 = float(np.mean(res**2)) #varianza
    corr = float(np.exp(0.5 * sigma2)) #factor de correcci√≥n del sesgo lognormal - corrige el sesgo introducido al volver de log a escala original.

    ypred = np.exp(ypred_log) * corr
    # eso hace que si el modelo del cuerpo predice un valor mayor al umbral, rec√≥rtalo (‚Äòclip‚Äô) justo por debajo de u

    # >>> m√©tricas <<<
    rmse = float(np.sqrt(mean_squared_error(yte, ypred)))
    mae  = float(mean_absolute_error(yte, ypred))
    bias = float((np.mean(ypred) - np.mean(yte)) / max(np.mean(yte), 1e-12))

    print(f"Cuerpo (‚â§ u={u:,}) ‚Äî RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | Bias: {100*bias:.2f}%")
    print(f"Var(log-residuos)‚âà{sigma2:.4f}  ‚áí  factor de correcci√≥n exp(0.5œÉ¬≤)={corr:.4f}")

    return model_fit, enc, cat_cols, num_cols, ypred, yte

# Filtrar SOLO cuerpo: 0 < SEVERITY ‚â§ u
body_train = train_data[(train_data['SEVERITY'] > 0)].copy()
body_test  = test_data[(test_data['SEVERITY'] > 0)].copy()
xgb_body_params = {
    'objective': 'reg:gamma',
    'learning_rate': 0.05,
    'max_depth': 6,
    'n_estimators': 600,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_lambda': 1.0,
    'random_state': 42
}
xgb_body_model = create_xgb_model(xgb_body_params)

# Entreno con splits body_train / body_test y el umbral elegido
model_body, enc_body, cat_cols_body, num_cols_body, preds_body, y_test_body = (
    train_test_body(xgb_body_model, body_train, body_test, u)
)
preds_body = preds_body[preds_body <= u]
preds_body.max()

simulated_excesses = genpareto.rvs(
    c=xi,               # 'c' es el par√°metro de forma xi en scipy
    loc=0.0,            # El exceso m√≠nimo es 0
    scale=beta,
    size= 230,
    random_state=8

)
simulated_tail_claims = simulated_excesses + u

print(f"\nPredicci√≥n M√≠nima de la Cola v√≠a Peaks Over Threshold: {np.min(simulated_tail_claims):,.2f}")
print(f"Predicci√≥n M√°xima de la Cola v√≠a Peaks Over Threshold: {np.max(simulated_tail_claims):,.2f}")

print(f"Predicci√≥n M√≠nima del modelo XGBOOST: {preds_body.min():.2f}")
print(f"Predicci√≥n M√°xima del modelo XGBOOST: {preds_body.max():.2f}")
print(f"Valor m√°ximo real: {df_final['SEVERITY'].max():.2f}")

"""VALOR ESPERADO DE SEVERIDAD MIXTA:

![image.png](data:image/png;base64,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)

# VALOR ESPERADO DE SEVERIDAD
"""

E_body = np.mean(preds_body)
print(f"valor esperado de severidad condicionado a que est√© dentro del grueso de la distribuci√≥n: {E_body:,.2f}")

# media te√≥rica de la cola bajo una distribuci√≥n Generalized Pareto (GPD)
def tail_mean(u, xi, beta):
    if xi >= 1:
        return np.inf
    return u + beta/(1 - xi)

E_tail = tail_mean(u, xi, beta)
print(f"valor esperado de severidad condicionado a que est√© dentro de la cola de la distribuci√≥n: {E_tail:,.2f}")

p_u

E_X = (1 - p_u) * E_body + p_u * E_tail
print(f"valor esperado de severidad descontextualizado: {E_X:,.2f}")
print(f"valor esperado de severidad real: {df_final[df_final['SEVERITY'] != 0]['SEVERITY'].mean():,.2f}")

"""# VALOR ESPERADO DE SINIESTRALIDAD (FRECUENCIA X SEVERIDAD)"""

E_S = E_X * valor_esp_frecuencia
print(f"Prima Pura -  valor esperado de siniestralidad por p√≥liza: {E_S:,.2f}")

E_X

print(f"Prima Pura -  valor esperado de siniestralidad por p√≥liza sin enfoque Peaks Over Threshold: {df_final[df_final['SEVERITY'] != 0]['SEVERITY'].mean() * valor_esp_frecuencia:,.2f}")

loading = E_S/(df_final[df_final['SEVERITY'] != 0]['SEVERITY'].mean() * valor_esp_frecuencia)-1
print(f"Margen - recargo que se le puede hacer a la prima para contemplar este riesgo de valor extremo: {loading:,.5f}")

"""# MEDIDAS DE RIESGO"""

print("Medidas de riesgo del modelo de distribuci√≥n h√≠brida con enfoque Peaks Over Threshold")
for p in [0.95, 0.99, 0.995]:
    v = var_pot(p, xi, beta, u, p_u)
    es = tvar_pot(p, xi, beta, u, p_u)
    label = f"{int(p*1000)/10:.1f}%"
    print(f"\nNivel {label}")
    if np.isnan(v):
        print("  (El cuant√≠l est√° por debajo del umbral: usar el modelo del cuerpo / emp√≠rico)")
    else:
        print(f"  VaR  = {v:,.2f}")
        print(f"  TVaR = {es:,.2f}   (Œî={es - v:,.2f} ‚â• 0)")

print("Medidas de riesgo del modelo de Machine Learning XGBoost puro")
VaR_dict = {}
TVaR_dict = {}
for p in [0.95, 0.99, 0.995]:
    VaR_p = np.percentile(preds_sev, 100 * p)
    TVaR_p = preds_sev[preds_sev >= VaR_p].mean()
    VaR_dict[p] = VaR_p
    TVaR_dict[p] = TVaR_p
    label = f"{int(p*1000)/10:.1f}%"
    print(f"\nNivel {label}")
    if np.isnan(v):
        print("  (El cuant√≠l est√° por debajo del umbral: usar el modelo del cuerpo / emp√≠rico)")
    else:
        print(f"  VaR  = {VaR_dict[p]:,.2f}")
        print(f"  TVaR = {TVaR_dict[p]:,.2f}")

u

preds_hybrid_full = np.concatenate([preds_sev, simulated_tail_claims])


preds_body_plot = preds_hybrid_full[preds_hybrid_full  <= threshold]
preds_tail_plot = preds_hybrid_full[preds_hybrid_full >  threshold]

real_body_plot = claims[claims <= threshold]
real_tail_plot = claims[claims >  threshold]

x_max = np.percentile(np.concatenate([preds_hybrid_full, claims]), 99.9)
bins_full = np.linspace(0, x_max, 300)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7), sharey=True)

# ====== Panel IZQ: H√çBRIDA (ML+EVT) ======
sns.histplot(preds_body_plot, bins=bins_full, kde=False, color='navy',
             label='Grueso de la distribuci√≥n', ax=ax1)
sns.histplot(preds_tail_plot, bins=bins_full, kde=False, color='lightslategray',
             label='Valores extremos con ajuste GPD', ax=ax1)

ax1.axvline(x=threshold, color='sienna', linestyle='--', linewidth=2,
            label=f'Umbral (${threshold:,.0f})')
ax1.set_xlabel('Monto de siniestro')
ax1.set_ylabel('Frecuencia')
ax1.set_title('Distribuci√≥n H√≠brida (ML + EVT)')
ax1.set_yscale('log')
ax1.grid(axis='y', alpha=0.9)
ax1.legend()
ax1.xaxis.set_major_formatter(FuncFormatter(human_format))
ax1.yaxis.set_major_formatter(FuncFormatter(human_format))
ax1.set_xlim(0, x_max)

# ====== Panel DER: REAL (SEVERITY > 0) ======
sns.histplot(real_body_plot, bins=bins_full, kde=False, color='navy',
             label='Grueso de la distribuci√≥n', ax=ax2)
sns.histplot(real_tail_plot, bins=bins_full, kde=False, color='indianred',
             label='Valores extremos reales', ax=ax2)

ax2.axvline(x=threshold, color='sienna', linestyle='--', linewidth=2,
            label=f'Umbral (${threshold:,.0f})')
ax2.set_xlabel('Monto de siniestro')
ax2.set_title('Distribuci√≥n Real (condicional) con Umbral')
ax2.set_yscale('log')
ax2.grid(axis='y', alpha=0.9)
ax2.legend()
ax2.xaxis.set_major_formatter(FuncFormatter(human_format))
ax2.set_xlim(0, x_max)

plt.tight_layout()
plt.show()

preds_hybrid_full = np.concatenate([preds_sev, simulated_tail_claims])


preds_body_plot = preds_hybrid_full[preds_hybrid_full  <= threshold]
preds_tail_plot = preds_hybrid_full[preds_hybrid_full >  threshold]

real_body_plot = claims[claims <= threshold]
real_tail_plot = claims[claims >  threshold]

x_max = np.percentile(np.concatenate([preds_hybrid_full, claims]), 99.9)
bins_full = np.linspace(0, x_max, 300)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7), sharey=True)

# ====== Panel IZQ: H√çBRIDA (ML+EVT) ======
sns.histplot(preds_body_plot, bins=bins_full, kde=False, color='navy',
             label='Grueso de la distribuci√≥n', ax=ax1)
sns.histplot(preds_tail_plot, bins=bins_full, kde=False, color='lightslategray',
             label='Valores extremos con ajuste GPD', ax=ax1)

ax1.axvline(x=threshold, color='sienna', linestyle='--', linewidth=2,
            label=f'Umbral (${threshold:,.0f})')
ax1.set_xlabel('Monto de siniestro')
ax1.set_ylabel('Frecuencia')
ax1.set_title('Distribuci√≥n H√≠brida (ML + EVT)')
ax1.set_yscale('log')
ax1.grid(axis='y', alpha=0.9)
ax1.legend()
ax1.xaxis.set_major_formatter(FuncFormatter(human_format))
ax1.yaxis.set_major_formatter(FuncFormatter(human_format))
ax1.set_xlim(0, x_max)

# ====== Panel DER: REAL (SEVERITY > 0) ======
sns.histplot(real_body_plot, bins=bins_full, kde=False, color='navy',
             label='Grueso de la distribuci√≥n', ax=ax2)
sns.histplot(real_tail_plot, bins=bins_full, kde=False, color='indianred',
             label='Valores extremos reales', ax=ax2)

ax2.axvline(x=threshold, color='sienna', linestyle='--', linewidth=2,
            label=f'Umbral (${threshold:,.0f})')
ax2.set_xlabel('Monto de siniestro')
ax2.set_title('Distribuci√≥n Real (condicional) con Umbral')
ax2.set_yscale('log')
ax2.grid(axis='y', alpha=0.9)
ax2.legend()
ax2.xaxis.set_major_formatter(FuncFormatter(human_format))
ax2.set_xlim(0, x_max)

plt.tight_layout()
plt.show()

from scipy.stats import skew,kurtosis

skew_real = skew(claims)
skew_model = skew(preds_hybrid_full)
kurt_real = kurtosis(claims)
kurt_model = kurtosis(preds_hybrid_full)

print(f"Asimetr√≠a real: {skew_real:.2f}")
print(f"Asimetr√≠a modelo: {skew_model:.2f}")
print(f"Curtosis real: {kurt_real:.2f}")
print(f"Curtosis modelo: {kurt_model:.2f}")

plt.figure(figsize=(8,5))
sns.kdeplot(claims, label="Real", log_scale=True)
sns.kdeplot(preds_hybrid_full, label="Modelo h√≠brido", log_scale=True)
plt.title("Comparaci√≥n de densidad (escala log)")
plt.xlabel("Monto de Severidad")
plt.legend()
plt.show()

print(preds_body)

# ===================== COTIZACI√ìN FINAL ‚Äî APLICACI√ìN PR√ÅCTICA =====================
nuevo_asegurado = pd.DataFrame([{
    "MVR_PTS": 3,
    "CLM_FREQ": 0,
    "OCCUPATION": "Blue Collar",
    "EDUCATION": "PhD",
    "PARENT1": "No",
    "REVOKED": "No",
    "MSTATUS": "Yes",
    "OLDCLAIM": 0.0,
    "HOMEKIDS": 0,
    "CAR_USE": "Private",
    "AGE": 35.0,
    "FREQUENCY": 0,
    "SEVERITY": 0.0
}])

# 2) Construir X con el mismo esquema que usaste al entrenar (num + cat codificadas)
cols_modelo = list(num_cols_body) + list(cat_cols_body)
X_nuevo = nuevo_asegurado[cols_modelo].copy()

Xn_num = X_nuevo[num_cols_body].copy()
Xn_cat = X_nuevo[cat_cols_body].copy()
Xn_cat_enc = enc_body.transform(Xn_cat)  # <<< usa tu encoder entrenado
X_nuevo_enc = pd.concat([Xn_num.reset_index(drop=True), Xn_cat_enc.reset_index(drop=True)], axis=1)

# 3) Predicci√≥n de FRECUENCIA (ya en escala final)
freq_pred = float(model_freq.predict(X_nuevo_enc)[0])

# 4) Predicci√≥n de SEVERIDAD (el modelo del cuerpo predice en LOG; aplicamos tu misma correcci√≥n)
#    a) armamos el X de entrenamiento con el mismo esquema (num + cat encod)
bt_num = body_train[num_cols_body].copy()
bt_cat = body_train[cat_cols_body].copy()
bt_cat_enc = enc_body.transform(bt_cat)
Xtr_enc_body = pd.concat([bt_num.reset_index(drop=True), bt_cat_enc.reset_index(drop=True)], axis=1)

#    b) residuales en log para estimar sigma^2 (como en tu train_test_body)
ytr_log_true = np.log(body_train["SEVERITY"].clip(lower=1e-6))
res_log = ytr_log_true.values - model_body.predict(Xtr_enc_body)
sigma2 = float(np.mean(res_log**2))
corr = float(np.exp(0.5 * sigma2))  # correcci√≥n lognormal de tu funci√≥n

#    c) predicci√≥n final en escala original (y clip opcional al umbral u del cuerpo)
sev_pred_log = float(model_body.predict(X_nuevo_enc)[0])
sev_pred = float(np.exp(sev_pred_log) * corr)
sev_pred = float(np.minimum(sev_pred, u))  # respeta el dominio del cuerpo (opcional, no cambia el modelo)

# 5) Primas
prima_pura = freq_pred * sev_pred
prima_con_loading = prima_pura * (1 + loading)

# 6) Resultado final (inputs + outputs)
cotizacion = nuevo_asegurado.copy()
cotizacion["freq_esperada"] = round(freq_pred, 5)
cotizacion["sev_esperada"] = round(sev_pred, 2)
cotizacion["prima_pura"] = round(prima_pura, 2)
cotizacion["prima_con_loading"] = round(prima_con_loading, 2)

print("=== Cotizaci√≥n estimada para el asegurado ===")
print(cotizacion[[
    "MVR_PTS","OCCUPATION","EDUCATION","AGE","CAR_USE",
    "freq_esperada","sev_esperada","prima_pura","prima_con_loading"
]])

# Filtrar registros reales con las mismas caracter√≠sticas
comparables = df_final[
    (df_final["OCCUPATION"] == "Blue Collar") &
    #(df_final["EDUCATION"] == "High School") &
    (df_final["AGE"] == 35) &
    (df_final["MVR_PTS"] == 3)
]

# Mostrar coincidencias
print(f"Se encontraron {len(comparables)} registros similares en df_final:")
display(comparables.head())

!pip install ipywidgets

# ===================== COTIZADOR CON WIDGETS  =====================
from IPython.display import display, clear_output
import ipywidgets as w

# --- helpers para opciones y rangos ---
def qminmax(s, qlo=0.01, qhi=0.99):
    s = pd.to_numeric(s, errors="coerce").dropna()
    if s.empty:
        return 0.0, 1.0
    lo, hi = float(s.quantile(qlo)), float(s.quantile(qhi))
    if not np.isfinite(lo) or not np.isfinite(hi) or lo >= hi:
        lo, hi = float(s.min()), float(s.max())
    return lo, hi

def options_from(col):
    vals = (body_train[col].dropna().astype(str).value_counts().index.tolist())[:50]
    return vals if vals else [""]

# --- widgets seg√∫n columnas ---
mvr_lo, mvr_hi = qminmax(body_train["MVR_PTS"])
old_lo, old_hi = qminmax(body_train["OLDCLAIM"])
age_lo, age_hi = qminmax(body_train["AGE"])
clm_lo, clm_hi = qminmax(body_train["CLM_FREQ"])

w_mvr     = w.IntSlider(description="MVR_PTS", min=int(mvr_lo), max=int(max(mvr_hi, mvr_lo+1)), step=1, value=3, continuous_update=False)
w_clm     = w.IntSlider(description="CLM_FREQ", min=int(clm_lo), max=int(max(clm_hi, clm_lo+1)), step=1, value=0, continuous_update=False)
w_occ     = w.Dropdown(description="OCCUPATION", options=options_from("OCCUPATION"), value="Blue Collar" if "Blue Collar" in options_from("OCCUPATION") else None)
w_edu     = w.Dropdown(description="EDUCATION",  options=options_from("EDUCATION"),  value="PhD" if "PhD" in options_from("EDUCATION") else None)
w_parent1 = w.Dropdown(description="PARENT1",    options=options_from("PARENT1"))
w_rev     = w.Dropdown(description="REVOKED",    options=options_from("REVOKED"))
w_mstat   = w.Dropdown(description="MSTATUS",    options=options_from("MSTATUS"))
w_old     = w.FloatSlider(description="OLDCLAIM", min=float(old_lo), max=float(old_hi), step=(old_hi-old_lo)/100 if old_hi>old_lo else 1.0, value=0.0, readout_format=".2f", continuous_update=False)
w_home    = w.IntSlider(description="HOMEKIDS", min=0, max=int(max(5, body_train["HOMEKIDS"].max() if "HOMEKIDS" in body_train else 5)), step=1, value=0, continuous_update=False)
w_use     = w.Dropdown(description="CAR_USE",    options=options_from("CAR_USE"), value="Private" if "Private" in options_from("CAR_USE") else None)
w_age     = w.IntSlider(description="AGE",       min=int(max(16, age_lo)), max=int(min(95, age_hi)), step=1, value=35, continuous_update=False)

btn = w.Button(description="Calcular cotizaci√≥n", button_style="primary")
out = w.Output()

# --- funci√≥n que usa  modelos y encoder directamente ---
def calcular(_):
    with out:
        clear_output()

        nuevo_asegurado = pd.DataFrame([{
            "MVR_PTS":   int(w_mvr.value),
            "CLM_FREQ":  int(w_clm.value),
            "OCCUPATION":str(w_occ.value),
            "EDUCATION": str(w_edu.value),
            "PARENT1":   str(w_parent1.value),
            "REVOKED":   str(w_rev.value),
            "MSTATUS":   str(w_mstat.value),
            "OLDCLAIM":  float(w_old.value),
            "HOMEKIDS":  int(w_home.value),
            "CAR_USE":   str(w_use.value),
            "AGE":       float(w_age.value),
            "FREQUENCY": 0,
            "SEVERITY":  0.0
        }])

        # === preparar input igual que en entrenamiento ===
        cols_modelo = list(num_cols_body) + list(cat_cols_body)
        X_nuevo = nuevo_asegurado[cols_modelo].copy()
        Xn_num = X_nuevo[num_cols_body].copy()
        Xn_cat = X_nuevo[cat_cols_body].copy()
        Xn_cat_enc = enc_body.transform(Xn_cat)
        X_nuevo_enc = pd.concat([Xn_num.reset_index(drop=True), Xn_cat_enc.reset_index(drop=True)], axis=1)

        # === predicciones ===
        freq_pred = float(model_freq.predict(X_nuevo_enc)[0])

        bt_num = body_train[num_cols_body].copy()
        bt_cat = body_train[cat_cols_body].copy()
        bt_cat_enc = enc_body.transform(bt_cat)
        Xtr_enc_body = pd.concat([bt_num.reset_index(drop=True), bt_cat_enc.reset_index(drop=True)], axis=1)

        ytr_log_true = np.log(body_train["SEVERITY"].clip(lower=1e-6))
        res_log = ytr_log_true.values - model_body.predict(Xtr_enc_body)
        sigma2 = float(np.mean(res_log**2))
        corr = float(np.exp(0.5 * sigma2))

        sev_pred_log = float(model_body.predict(X_nuevo_enc)[0])
        sev_pred = float(np.exp(sev_pred_log) * corr)
        try:
            sev_pred = float(np.minimum(sev_pred, u))
        except NameError:
            pass

        # === primas (usa tu loading definido antes) ===
        prima_pura = freq_pred * sev_pred
        prima_con_loading = prima_pura * (1 + loading)

        cotizacion = nuevo_asegurado.copy()
        cotizacion["freq_esperada"] = round(freq_pred, 5)
        cotizacion["sev_esperada"] = round(sev_pred, 2)
        cotizacion["prima_pura"] = round(prima_pura, 2)
        cotizacion["prima_con_loading"] = round(prima_con_loading, 2)
        col_map = {
            "MVR_PTS": "Puntuaci√≥n del conductor",
            "CLM_FREQ": "# Siniestros (√∫ltimos 5 a√±os)",
            "OCCUPATION": "Profesi√≥n",
            "EDUCATION": "Nivel educativo",
            "PARENT1": "Tiene hijos",
            "REVOKED": "Licencia suspendida",
            "MSTATUS": "Estado civil",
            "OLDCLAIM": "Monto siniestros previos",
            "HOMEKIDS": "Cantidad de hijos",
            "CAR_USE": "Uso del veh√≠culo",
            "AGE": "Edad"
        }

        cols_es = [
            "Puntuaci√≥n del conductor","Profesi√≥n","Nivel educativo","Edad","Uso del veh√≠culo",
            "freq_esperada","sev_esperada","prima_pura","prima_con_loading"
        ]
        cotizacion_view = cotizacion.rename(columns=col_map)
        display(cotizacion_view[cols_es])


btn.on_click(calcular)

# --- mostrar interfaz ---
ui_left  = w.VBox([w_mvr, w_clm, w_old, w_home, w_age])
ui_right = w.VBox([w_occ, w_edu, w_parent1, w_rev, w_mstat, w_use])
display(w.HBox([ui_left, w.Label("   "), ui_right]))
display(btn, out)
# ===================== FIN COTIZADOR CON WIDGETS =====================